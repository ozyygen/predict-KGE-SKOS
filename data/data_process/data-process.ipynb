{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17abf2e7-25c3-416a-8d35-8672e3e57e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "from io import TextIOWrapper\n",
    "\n",
    "def parse_rrf_data_line(line):\n",
    "    # Split the line using the '|' delimiter\n",
    "    values = line.strip().split('|')\n",
    "\n",
    "    # Create a dictionary with column names and values\n",
    "    data_dict = {f'Col_{i+1}': val for i, val in enumerate(values)}\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "def process_files_in_zip(zip_file_path,f_name):\n",
    "    parsed_data = []  # List to store parsed content as DataFrames\n",
    "\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        for file_info in zip_ref.infolist():\n",
    "            file_name = file_info.filename\n",
    "\n",
    "            if file_name.endswith('.RRF') and (file_name.startswith(f_name) ):\n",
    "                print(file_name)\n",
    "                with zip_ref.open(file_name) as file:\n",
    "                    # Process the RRF file line by line using your parser function\n",
    "                    parsed_content = []\n",
    "                    with TextIOWrapper(file, encoding='utf-8') as text_file:\n",
    "                        for line in text_file:\n",
    "                            parsed_line = parse_rrf_data_line(line)\n",
    "                            parsed_content.append(parsed_line)\n",
    "\n",
    "                    # Convert the list of dictionaries to a DataFrame\n",
    "                    parsed_data = pd.DataFrame(parsed_content)\n",
    "\n",
    "\n",
    "\n",
    "    return parsed_data  # Return the list of parsed content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f94caa-8615-43d0-8447-6ba67159d926",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_path = '/home/jovyan/work/umls-2023AB-metathesaurus-full.zip'\n",
    "file_n = '2023AB/META/MRCONSO.RRF'\n",
    "parsed_data_name = process_files_in_zip(zip_file_path,file_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0389b0-6c61-47e6-9f90-5365fa082060",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data_name = parsed_data_name[parsed_data_name['Col_2'] == 'ENG']\n",
    "columns_to_keep = [0, 7, 14]\n",
    "parsed_data_name = parsed_data_name.iloc[:, columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e778d794-7d7f-4daa-b6eb-9ac6fbd1f9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_path = '/home/jovyan/work/umls-2023AB-metathesaurus-full.zip'\n",
    "file_n = '2023AB/META/MRHIER.RRF'\n",
    "parsed_data_hier = process_files_in_zip(zip_file_path,file_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4627229-fdfa-4174-8a78-7b32a9c3d50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [0, 1, 6]\n",
    "parsed_data_hier = parsed_data_hier.iloc[:, columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2428936-c962-4eef-8010-f5febe0798bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hier_no_nan = parsed_data_hier[parsed_data_hier['Col_7'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ca9520-521f-4b3b-a445-61a707d16605",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data_hier.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afafdead-d023-4668-bb26-45aeb406be46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rdfpandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9afd97b-ed82-4b3f-b615-27fea2b90ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rdflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6c0d1c-24ed-4383-a4f2-e6ba61834445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "import pandas as pd\n",
    "from rdfpandas.graph import to_dataframe\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4756a573-3b88-4c02-ae56-0a56eb7654d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ttl_files(pathfilename):\n",
    "    g = rdflib.Graph()\n",
    "    g.parse(pathfilename)\n",
    "    data = []\n",
    "\n",
    "    for subject, predicate, obj in g:\n",
    "        data.append((subject, predicate, obj))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['s', 'p', 'o'])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0347a011-2f08-40db-8667-7a7072c6bea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_hierarchy_associative_clash(df):\n",
    "    violating_dict = defaultdict(list)\n",
    "    related = rdflib.URIRef(\"http://www.w3.org/2004/02/skos/core#related\")\n",
    "    broader = rdflib.URIRef(\"http://www.w3.org/2004/02/skos/core#broader\")\n",
    "\n",
    "    # Create a set for faster membership checks\n",
    "    df_set = {(row.s, row.p, row.o) for row in df.itertuples(index=False)}\n",
    "\n",
    "    # Create an inverted index for related concepts\n",
    "    related_concepts = defaultdict(set)\n",
    "    broader_concepts = defaultdict(set)\n",
    "    for row in df_set:\n",
    "        concept, relation, other_concept = row\n",
    "        if relation == related:\n",
    "            related_concepts[concept].add((concept, relation, other_concept))\n",
    "        if relation == broader:\n",
    "            broader_concepts[concept].add((concept, relation, other_concept))\n",
    "\n",
    "    for row in df_set:\n",
    "        concept, relation, other_concept = row\n",
    "\n",
    "        if relation == related:\n",
    "            if (concept in broader_concepts and (concept, broader, other_concept) in broader_concepts[concept]) or (other_concept in broader_concepts and (other_concept, broader, concept) in broader_concepts[other_concept]) :\n",
    "                violating_dict[concept].append((concept, relation, other_concept))\n",
    "                violating_dict[other_concept].append((concept, broader, other_concept))\n",
    "                df = df[~((df['s'] == concept) & (df['p'] == relation) & (df['o'] == other_concept))].reset_index(drop=True)\n",
    "\n",
    "        elif relation == broader:\n",
    "            if (concept in related_concepts and (concept, related, other_concept) in related_concepts[concept]) or (other_concept in related_concepts and (other_concept, related, concept) in related_concepts[other_concept]) :\n",
    "                violating_dict[concept].append((concept, relation, other_concept))\n",
    "                violating_dict[concept].append((concept, related, other_concept))\n",
    "                df = df[~((df['s'] == concept) & (df['p'] == relation) & (df['o'] == other_concept))].reset_index(drop=True)\n",
    "\n",
    "    if violating_dict:\n",
    "        violating_df = pd.DataFrame([item for sublist in violating_dict.values() for item in sublist], columns=['s', 'p', 'o'])\n",
    "        return df, violating_df\n",
    "    else:\n",
    "        print(\"Hierarchy is consistent in terms of hierarchical & associative links clashes\")\n",
    "        return df, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb0a3bd-8bb7-444c-81e2-699faf8387ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_and_update_kg(kg_df, negative_df):\n",
    "\n",
    "    for _, violating_triple in negative_df.iterrows():\n",
    "        s = violating_triple['s']\n",
    "        p = violating_triple['p']\n",
    "        o = violating_triple['o']\n",
    "        kg_df = kg_df[~((kg_df['s'] == s) & (kg_df['p'] == p) & (kg_df['o'] == o))].reset_index(drop=True)\n",
    "\n",
    "       \n",
    "\n",
    "    return kg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136a39ae-a22b-45d2-9081-194ec2c728ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train,test and valid set splits\n",
    "def split_dataset(df):\n",
    "    df_shuffled = df.sample(frac=1, random_state=42)\n",
    "\n",
    "    total_samples = df_shuffled.shape[0]\n",
    "    train_size = int(0.8 * total_samples)\n",
    "    valid_size = int(0.1 * total_samples)\n",
    "    test_size = total_samples - train_size - valid_size\n",
    "\n",
    "    df_train = df_shuffled.iloc[:train_size]\n",
    "    df_valid = df_shuffled.iloc[train_size:train_size + valid_size]\n",
    "    df_test = df_shuffled.iloc[train_size + valid_size:]\n",
    "\n",
    "    return df_train, df_valid, df_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da171a7b-943f-44c3-a8b5-ceafbfec03c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_id_files(df):\n",
    "    entities_s = pd.DataFrame({'e':[]})\n",
    "    entities_o = pd.DataFrame({'e':[]})\n",
    "    # Extract unique entities and relations\n",
    "    entities_s['e'] = df['s']\n",
    "    entities_o['e'] = df['o']\n",
    "    entities_all = pd.concat([entities_s ,entities_o],ignore_index=True).reset_index()\n",
    "    relations = df['p'].drop_duplicates()\n",
    "    entities = entities_all['e'].drop_duplicates()\n",
    "\n",
    "    e_to_id = {}\n",
    "    rel_to_id = {}\n",
    "\n",
    "    # Save entity IDs to a text file\n",
    "    with open('entities.dict', 'w') as entity_file:\n",
    "\n",
    "        for idx, entity in enumerate(entities):\n",
    "            entity_file.write(f\"{idx}\\t{entity}\\n\")\n",
    "            e_to_id[entity] = idx\n",
    "\n",
    "    # Save relation IDs to a text file\n",
    "    with open('relations.dict', 'w') as relation_file:\n",
    "\n",
    "        for idx, relation in enumerate(relations):\n",
    "            relation_file.write(f\"{idx}\\t{relation}\\n\")\n",
    "            rel_to_id[relation] = idx\n",
    "    return e_to_id, rel_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d05c7d-da12-46db-af26-c852cc937286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_triples_to_files(entity_to_id, relation_to_id,text_df,df):\n",
    "\n",
    "    # Save triple IDs to a text file\n",
    "    with open(text_df, 'w') as triples_file:\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            subject =row['s']\n",
    "            relation = row['p']\n",
    "            o = row['o']\n",
    "            triples_file.write(f\"{subject}\\t{relation}\\t{o}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08331378-e4b0-439e-92fc-7b95a17d5ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = load_ttl_files(\"/home/jovyan/work/Medicine_allTriples.ttl\")\n",
    "updated_kg, negative_df = has_hierarchy_associative_clash(all_df)\n",
    "updated_kg, negative_df_s2 = has_hierarchy_associative_clash_s2(all_df)\n",
    "e_to_id, rel_to_id = convert_to_id_files(all_df)\n",
    "df_train, df_valid, df_test = split_dataset (updated_kg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80aff16-2454-4356-b140-3d51fee7cd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\"s\", \"o\"]\n",
    "\n",
    "# Extracting values from selected columns\n",
    "selected_values = negative_df[selected_columns].values\n",
    "\n",
    "# Creating a homogeneous list by flattening the array\n",
    "homogeneous_list = selected_values.flatten().tolist()\n",
    "\n",
    "# Creating a list with unique values\n",
    "unique_values_list = list(set(homogeneous_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438e9a4c-474f-4931-b87d-71c40a9395dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, URIRef, Literal, Namespace\n",
    "from rdflib.plugins.sparql import prepareQuery\n",
    "data = []\n",
    "\n",
    "g = rdflib.Graph()\n",
    "g.parse(\"/home/jovyan/work/Medicine_allTriples.ttl\")\n",
    "# Define SKOS namespace\n",
    "skos = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
    "\n",
    "for item in unique_values_list:\n",
    "\n",
    "# SPARQL query to get skos:Concept and skos:prefLabel for the given concept URI\n",
    "    sparql_query = \"\"\"\n",
    "    SELECT ?concept ?prefLabel\n",
    "    WHERE {\n",
    "        ?concept_uri a <http://www.w3.org/2004/02/skos/core#Concept> ;\n",
    "                 <http://www.w3.org/2004/02/skos/core#prefLabel> ?prefLabel .\n",
    "    }\n",
    "\"\"\"\n",
    "    results = g.query(sparql_query, initBindings={'concept_uri': item})\n",
    "\n",
    "\n",
    "\n",
    "# the results\n",
    "    for row in results:\n",
    "        concept = item\n",
    "        prefLabel = row[\"prefLabel\"]\n",
    "        data.append((concept,prefLabel))\n",
    "df_med = pd.DataFrame(data, columns=['Id','STR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e944d436-4a68-4586-b418-4aa773f318dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data_name = parsed_data_name.rename(columns={'Col_15': 'STR'})\n",
    "parsed_data_name.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b825c9-8509-4560-bb27-74444d6af223",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167946c7-ef19-4cc4-9323-230b1da9bc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed89793a-81f3-412c-9dc7-a8a9e3b219aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from scipy.spatial.distance import cosine, jaccard\n",
    "import distance\n",
    "\n",
    "def evaluate_matching_methods(df1, df2):\n",
    "    key_column_name_df1 = 'STR'\n",
    "    key_column_name_df2 = 'STR'\n",
    "\n",
    "    results = pd.DataFrame()\n",
    "\n",
    "    # Compute Levenshtein Distance\n",
    "    results['Levenshtein_Distance'] = df1.apply(lambda row1: df2[key_column_name_df2].apply(lambda row2: fuzz.ratio(row1[key_column_name_df1], row2)), axis=1)\n",
    "\n",
    "    # Compute Jaro-Winkler\n",
    "    results['Jaro_Winkler'] = df1.apply(lambda row1: df2[key_column_name_df2].apply(lambda row2: fuzz.jaro_winkler(row1[key_column_name_df1], row2)), axis=1)\n",
    "\n",
    "    # Compute Jaccard Similarity using scipy\n",
    "    tokenize = lambda text: set(text.lower().split())\n",
    "    results['Jaccard_Similarity'] = df1.apply(lambda row1: df2[key_column_name_df2].apply(lambda row2: 1 - jaccard(tokenize(row1[key_column_name_df1]), tokenize(row2[key_column_name_df2]))), axis=1)\n",
    "\n",
    "    # Compute Cosine Similarity using scipy\n",
    "    vectors_df1 = df1[key_column_name_df1].apply(tokenize)\n",
    "    vectors_df2 = df2[key_column_name_df2].apply(tokenize)\n",
    "    results['Cosine_Similarity'] = df1.apply(lambda row1: df2.apply(lambda row2: 1 - cosine(vectors_df1[row1.name], vectors_df2[row2.name])), axis=1)\n",
    "\n",
    "    return results\n",
    "\n",
    "# usage:\n",
    "evaluation_results = evaluate_matching_methods(parsed_data_name, df_med)\n",
    "print(\"Evaluation Results:\")\n",
    "print(evaluation_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
